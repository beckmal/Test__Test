"""
Utility script to analyze augmentation metadata generated by Augment_Sets_Tracked.jl

Usage:
  julia --script Analyze_Augmentation_Metadata.jl

This script loads the augmentation metadata and generates:
  1. Source image usage statistics
  2. Quality metrics distributions
  3. Visualization plots
  4. CSV export of metadata
"""

import Pkg
Pkg.activate(@__DIR__)

using Bas3ImageSegmentation
using Bas3ImageSegmentation.JLD2
using Statistics
using Dates

# Path setup
function resolve_path(relative_path::String)
    if Sys.iswindows()
        if startswith(relative_path, "/mnt/")
            drive_letter = uppercase(relative_path[6])
            rest_of_path = replace(relative_path[8:end], "/" => "\\")
            return "$(drive_letter):\\$(rest_of_path)"
        else
            return relative_path
        end
    else
        if occursin(r"^[A-Za-z]:[/\\]", relative_path)
            drive_letter = lowercase(relative_path[1])
            rest_of_path = replace(relative_path[4:end], "\\" => "/")
            return "/mnt/$(drive_letter)/$(rest_of_path)"
        else
            return relative_path
        end
    end
end

base_path = resolve_path("C:/Syncthing/Datasets")
metadata_dir = joinpath(base_path, "augmented_metadata")
summary_file = joinpath(metadata_dir, "augmentation_summary.jld2")

println("\n" * "="^80)
println("AUGMENTATION METADATA ANALYZER")
println("="^80)

# Check if metadata exists
if !isfile(summary_file)
    println("ERROR: Metadata file not found at: $(summary_file)")
    println("Please run Augment_Sets_Tracked.jl first to generate metadata.")
    exit(1)
end

println("Loading metadata from: $(summary_file)")

# Load metadata
data = JLD2.load(summary_file)
all_metadata = data["all_metadata"]
source_usage_counts = data["source_usage_counts"]
selection_history = data["selection_history"]
total_selections = data["total_selections"]
excluded_indices = data["excluded_indices"]
generation_timestamp = data["generation_timestamp"]

println("✓ Metadata loaded successfully")
println("  Generation timestamp: $(generation_timestamp)")
println("  Total augmented samples: $(length(all_metadata))")
println("  Total source selections: $(total_selections)")

# ============================================================================
# 1. SOURCE IMAGE USAGE ANALYSIS
# ============================================================================

println("\n" * "="^80)
println("SOURCE IMAGE USAGE ANALYSIS")
println("="^80)

println("\nDetailed usage statistics:")
println(lpad("Source", 10), lpad("Count", 10), lpad("Percentage", 15), "  ", "Bar")
println("-"^80)

max_count = maximum(source_usage_counts)
for (idx, count) in enumerate(source_usage_counts)
    if count > 0
        pct = count / sum(source_usage_counts) * 100
        bar_length = round(Int, count / max_count * 40)
        bar = "█"^bar_length
        excluded_marker = idx in excluded_indices ? " [EXCLUDED]" : ""
        println(
            lpad("#$idx", 10), 
            lpad(count, 10), 
            lpad("$(round(pct, digits=2))%", 15),
            "  ",
            bar,
            excluded_marker
        )
    end
end

# Statistics
used_sources = count(>(0), source_usage_counts)
avg_usage = mean(filter(>(0), source_usage_counts))
std_usage = std(filter(>(0), source_usage_counts))
min_usage = minimum(filter(>(0), source_usage_counts))
max_usage = maximum(source_usage_counts)

println("\nSummary:")
println("  Sources used: $(used_sources) / $(length(source_usage_counts))")
println("  Average usage per source: $(round(avg_usage, digits=2))")
println("  Std dev: $(round(std_usage, digits=2))")
println("  Min usage: $(min_usage)")
println("  Max usage: $(max_usage)")
println("  Usage uniformity: $(round((1 - std_usage/avg_usage) * 100, digits=1))% (100% = perfectly uniform)")

# ============================================================================
# 2. QUALITY METRICS ANALYSIS
# ============================================================================

println("\n" * "="^80)
println("QUALITY METRICS ANALYSIS")
println("="^80)

# Extract metrics
scar_pcts = [m.scar_percentage for m in all_metadata]
redness_pcts = [m.redness_percentage for m in all_metadata]
hematoma_pcts = [m.hematoma_percentage for m in all_metadata]
necrosis_pcts = [m.necrosis_percentage for m in all_metadata]
background_pcts = [m.background_percentage for m in all_metadata]

# Display statistics
classes = [
    ("Scar", scar_pcts),
    ("Redness", redness_pcts),
    ("Hematoma", hematoma_pcts),
    ("Necrosis", necrosis_pcts),
    ("Background", background_pcts)
]

println("\nClass coverage statistics:")
println(lpad("Class", 15), lpad("Mean", 12), lpad("Std Dev", 12), lpad("Min", 10), lpad("Max", 10), lpad("<5%", 10))
println("-"^80)

for (class_name, percentages) in classes
    below_5_count = count(<(5), percentages)
    below_5_pct = round(below_5_count / length(percentages) * 100, digits=1)
    
    println(
        lpad(class_name, 15),
        lpad("$(round(mean(percentages), digits=2))%", 12),
        lpad("$(round(std(percentages), digits=2))%", 12),
        lpad("$(round(minimum(percentages), digits=2))%", 10),
        lpad("$(round(maximum(percentages), digits=2))%", 10),
        lpad("$(below_5_count) ($(below_5_pct)%)", 10)
    )
end

println("\nNote: '<5%' column shows samples that would have been rejected by the old pipeline")

# ============================================================================
# 3. TEMPORAL ANALYSIS
# ============================================================================

println("\n" * "="^80)
println("TEMPORAL ANALYSIS")
println("="^80)

timestamps = [m.timestamp for m in all_metadata]
if length(timestamps) > 1
    duration = timestamps[end] - timestamps[1]
    
    println("  First sample: $(timestamps[1])")
    println("  Last sample:  $(timestamps[end])")
    println("  Total duration: $(duration)")
    if duration.value > 0
        avg_ms_per_sample = duration.value / length(timestamps)
        println("  Average time per sample: $(round(avg_ms_per_sample, digits=2)) ms")
    end
end

# ============================================================================
# 4. REPRODUCIBILITY INFORMATION
# ============================================================================

println("\n" * "="^80)
println("REPRODUCIBILITY INFORMATION")
println("="^80)

println("\nRandom seeds stored: $(length(all_metadata))")
println("Sample metadata entries:")
for i in 1:min(3, length(all_metadata))
    m = all_metadata[i]
    println("  [$(i)] Source: #$(m.source_index), Seed: $(m.random_seed), Scar: $(round(m.scar_percentage, digits=1))%")
end

# ============================================================================
# 5. EXPORT TO CSV
# ============================================================================

println("\n" * "="^80)
println("EXPORTING METADATA TO CSV")
println("="^80)

csv_file = joinpath(metadata_dir, "augmentation_metadata.csv")
println("Writing to: $(csv_file)")

open(csv_file, "w") do io
    # Header
    println(io, "augmented_index,source_index,timestamp,random_seed,scar_pct,redness_pct,hematoma_pct,necrosis_pct,background_pct")
    
    # Data rows
    for m in all_metadata
        println(io, "$(m.augmented_index),$(m.source_index),$(m.timestamp),$(m.random_seed),$(m.scar_percentage),$(m.redness_percentage),$(m.hematoma_percentage),$(m.necrosis_percentage),$(m.background_percentage)")
    end
end

println("✓ CSV export complete: $(csv_file)")

# ============================================================================
# 6. SOURCE USAGE CSV
# ============================================================================

usage_csv_file = joinpath(metadata_dir, "source_usage.csv")
println("\nWriting source usage to: $(usage_csv_file)")

open(usage_csv_file, "w") do io
    println(io, "source_index,usage_count,percentage,excluded")
    for (idx, count) in enumerate(source_usage_counts)
        pct = count / sum(source_usage_counts) * 100
        excluded = idx in excluded_indices
        println(io, "$(idx),$(count),$(pct),$(excluded)")
    end
end

println("✓ Source usage CSV export complete: $(usage_csv_file)")

# ============================================================================
# 7. COMPARISON WITH OLD PIPELINE
# ============================================================================

println("\n" * "="^80)
println("COMPARISON WITH OLD PIPELINE")
println("="^80)

println("\nOld pipeline issues:")
println("  ✗ Class oversampling: Scar 6x, Necrosis 3x, Redness 5x, Hematoma 1x")
println("  ✗ Rejection loops: Unknown number of rejected samples")
println("  ✗ No metadata tracking")
println("  ✗ Hardcoded exclusions: #8, #16")
println("  ✗ Non-uniform source distribution")

println("\nNew pipeline improvements:")
println("  ✓ Uniform source selection (round-robin)")
println("  ✓ No rejection loops (all samples accepted)")
println("  ✓ Full metadata tracking (source, seed, timestamps)")
println("  ✓ Configurable exclusions: $(collect(excluded_indices))")
println("  ✓ Quality metrics computed but not used for filtering")

total_below_5 = sum([
    count(<(5), scar_pcts),
    count(<(5), redness_pcts),
    count(<(5), hematoma_pcts),
    count(<(5), necrosis_pcts)
])

println("\nEfficiency comparison:")
println("  Samples generated: $(length(all_metadata))")
println("  Samples that would be rejected (old): $(total_below_5) total class-checks with <5%")
println("  Samples accepted (new): $(length(all_metadata)) (100%)")
println("  Computational waste avoided: $(round(total_below_5 / (length(all_metadata) * 4) * 100, digits=1))% of class checks")

# ============================================================================
# 8. RECOMMENDATIONS
# ============================================================================

println("\n" * "="^80)
println("RECOMMENDATIONS")
println("="^80)

println("\n1. Post-hoc filtering:")
println("   If needed, filter samples after generation based on quality metrics")
println("   Example: Filter samples with <5% foreground for specific classes")

println("\n2. Resampling strategy:")
println("   If class imbalance is an issue, implement weighted sampling during training")
println("   rather than biasing augmentation generation")

println("\n3. Source image investigation:")
if any(source_usage_counts .== 0)
    unused = findall(==(0), source_usage_counts)
    println("   WARNING: $(length(unused)) source images were never used")
    println("   Indices: $(unused)")
    println("   Check if these should be added to excluded list")
else
    println("   ✓ All source images were used at least once")
end

println("\n4. Metadata usage:")
println("   - Use random_seed to reproduce specific augmentations")
println("   - Use source_index to track which originals were most/least used")
println("   - Use quality metrics to analyze class distribution")

println("\n" * "="^80)
println("ANALYSIS COMPLETE")
println("="^80)
println("\nFiles generated:")
println("  - $(csv_file)")
println("  - $(usage_csv_file)")
println("\nNext steps:")
println("  1. Review CSV files for detailed analysis")
println("  2. Consider implementing post-hoc filtering if needed")
println("  3. Use metadata for reproducibility and debugging")
println("="^80)
